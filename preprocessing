t=pd.DataFrame(data_real.title)
t['result']=0
r=pd.DataFrame(data_fake.title)
r['result']=1
s=t.append(r)
y1=s.dropna()
texts=y1['title']
y=y1['result']
y1
print(y1.groupby(['result'])['title'].count())
y1.groupby(['result'])['title'].count().plot(kind="bar")
plt.show()

stop_words = stopwords.words('english')

def preprocess(text):
    text = text.lower()
    doc = word_tokenize(text)
    doc = [word for word in doc if word not in stop_words]
    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only
    return doc
    
 corpus=[preprocess(text) for text in texts]
 def filter_docs(corpus, texts, labels, condition_on_doc):
    """
    Filter corpus, texts and labels given the function condition_on_doc which takes
    a doc.
    The document doc is kept if condition_on_doc(doc) is true.
    """
    number_of_docs = len(corpus)

    if texts is not None:
        texts = [text for (text, doc) in zip(texts, corpus)
                 if condition_on_doc(doc)]

    labels = [i for (i, doc) in zip(labels, corpus) if condition_on_doc(doc)]
    corpus = [doc for doc in corpus if condition_on_doc(doc)]

    print("{} docs removed".format(number_of_docs - len(corpus)))

    return (corpus, texts, labels)
  
  def document_vector(word2vec_model, doc):
    # remove out-of-vocabulary words
    doc = [word for word in doc if word in word2vec_model.vocab]
    return np.mean(word2vec_model[doc], axis=0)
    
  def has_vector_representation(word2vec_model, doc):
    """check if at least one word of the document is in the
    word2vec dictionary"""
    return not all(word not in word2vec_model.vocab for word in doc)
  
  corpus, texts, y = filter_docs(corpus, texts, y, lambda doc: has_vector_representation(glove_model, doc))
  
  x =[]
for doc in corpus: #look up each doc in model
    x.append(document_vector(glove_model, doc))
